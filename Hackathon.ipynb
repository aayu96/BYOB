{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILIpdYUTVoEq",
        "outputId": "26991df7-d531-494a-a18c-ce874d7e2b26"
      },
      "outputs": [],
      "source": [
        "pip install gtts pydub moviepy opencv-python-headless openai Pillow==9.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqGOnHS7VqZq"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "import moviepy.editor as mp\n",
        "import json\n",
        "from gtts import gTTS\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
        "\n",
        "openai.api_key = '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'\n",
        "model_id = 'gpt-3.5-turbo-16k'\n",
        "font_size = 43\n",
        "width, height = 1920, 1080\n",
        "font_color = (255, 255, 255)\n",
        "font_path = 'Itim-Regular.ttf'\n",
        "output_folder = 'whiteboard_images'\n",
        "words_per_line = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVcJBVqYUOTB"
      },
      "outputs": [],
      "source": [
        "# # AWS CLI command to copy the file\n",
        "# command = \"aws s3 cp s3://pw-hackathon-byob/lectures/acidbase_short.mp4 .\"\n",
        "\n",
        "# # Run the AWS CLI command\n",
        "# result = subprocess.run(command, shell=True)\n",
        "\n",
        "# # Check if the command was successful (return code 0)\n",
        "# if result.returncode == 0:\n",
        "#     # Use MoviePy to work with the video file\n",
        "#     my_clip = mp.VideoFileClip(\"acidbase_short.mp4\")\n",
        "#     my_clip.audio.write_audiofile(r\"audio.wav\")\n",
        "# else:\n",
        "#     print(\"Error copying the file from S3.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_clip = mp.VideoFileClip(\"./Theory_Summary/video6_theory_summary.mp4\")\n",
        "my_clip.audio.write_audiofile(r\"audio.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2rBYLw2WJw6"
      },
      "outputs": [],
      "source": [
        "prompt_part_summary = \"\"\"You are a subject matter expert ,which is helping student in revising for comeptitve exams(JEE,NEET). \n",
        "    You will be provided a part of lecture transcript and you need to summarise the most important parts of that transpict in no more than 1000 words. \n",
        "    Some guidelines for summarisation:-\n",
        "    1. The transpict will be in english, hindi and hinglish. The summary should only be in English. \n",
        "    2. The teacher will sometimes talk about topics not related to the subject ,to keep the students engaged. Do not include that in the summary. \n",
        "    4.Include key formulas in the summary as well.\n",
        "    The summary should help the student in fast revision by covering all the important points of the lecture in short ,crisp and to the point way.\n",
        "\n",
        "    \n",
        "    \n",
        "    The transcript that you will summarise is as follows:-\n",
        "    ----------\n",
        "    {}\n",
        "    ----------\n",
        "    \n",
        "    Note:- The output should just contain summary text and nothing else.\n",
        "    \n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "prompt_final_summary =\"\"\"You are a subject matter expert ,which is helping student in revising for competitve exams. \n",
        "    You will be provided a short description of the topic ,you need to summarise the most important parts of that topics from the text in no more than 500 words. \n",
        "    Include key formulas in the summary. Summarise the concepts and important points.Cover all the importnt points .Include relevant formulas.\n",
        "    Once you get the idea what the topic is about, focus on that topic and ignore anyhting else present.Do no include text that is not related tot he topics present in the text provided.\n",
        "    The summary should help the student in fast revision by covering all the important points of the lecture in short ,crisp and to the point way.\n",
        "    Sometimes different topics might be present, try to cover all the topics.\n",
        "\n",
        "      \n",
        "  --------\n",
        "  {}\n",
        "  --------\n",
        "  Note :- the summary should be divided into 4 parts and each part should not have more than 120 words. Also the summary should be in JSON Format as below:\n",
        "  Part1:\n",
        "  Part2:\n",
        "  Part3:\n",
        "  Part4:\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqVsMM91pS7u"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_id_transcript = 'whisper-1'\n",
        "\n",
        "media_file_path = 'audio.wav'\n",
        "chunk_size_seconds = 60  # To be less than 25MB\n",
        "\n",
        "# Function to transcribe audio chunks\n",
        "def transcribe_audio_chunk(api_key, model_id, chunk_path):\n",
        "    with open(chunk_path, 'rb') as file:\n",
        "        response = openai.Audio.transcribe(\n",
        "            api_key=api_key,\n",
        "            model=model_id_transcript,\n",
        "            file=file\n",
        "        )\n",
        "    return response['text']\n",
        "\n",
        "# Function to split audio into chunks\n",
        "def split_audio_into_chunks(file_path, chunk_size):\n",
        "    #temporary directory to store audio chunks\n",
        "    temp_dir = 'temp_audio_chunks'\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Get audio duration in seconds using pydub\n",
        "    audio = AudioSegment.from_wav(file_path)\n",
        "    audio_duration = len(audio) / 1000  # Convert milliseconds to seconds\n",
        "\n",
        "    # Calculate number of chunks\n",
        "    num_chunks = int(audio_duration / chunk_size) + 1\n",
        "\n",
        "    # Split audio into chunks\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start_time = i * chunk_size\n",
        "        end_time = min((i + 1) * chunk_size, audio_duration)\n",
        "\n",
        "        # Create a temporary chunk file\n",
        "        chunk_path = os.path.join(temp_dir, f'chunk_{i}.wav')  # Adjust the format if needed\n",
        "\n",
        "        # Use pydub to trim the audio and write it to a new file\n",
        "        chunk_audio = audio[start_time * 1000:end_time * 1000]\n",
        "        chunk_audio.export(chunk_path, format=\"wav\")\n",
        "\n",
        "        chunks.append(chunk_path)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ChatGPT_conversation(conversation):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model_id,\n",
        "        messages=conversation,\n",
        "        temperature =0\n",
        "    )\n",
        "    # api_usage = response['usage']\n",
        "    # print('Total token consumed: {0}'.format(api_usage['total_tokens']))\n",
        "    # stop means complete\n",
        "    # print(response['choices'][0].finish_reason)\n",
        "    # print(response['choices'][0].index)\n",
        "    #conversation.append({'role': response.choices[0].message.role, 'content': response.choices[0].message.content})\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_text(text, prompt, max_tokens=15000):\n",
        "    # Calculate tokens in the prompt\n",
        "    prompt_tokens = len(openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_part_summary)[\"choices\"][0][\"text\"].split())\n",
        "\n",
        "    # Split the remaining text into chunks with max_tokens limit\n",
        "    remaining_text = text\n",
        "    chunks = []\n",
        "    while len(remaining_text.split()) > 0:\n",
        "        chunk = remaining_text[:max_tokens - prompt_tokens]\n",
        "        chunks.append(prompt + chunk)\n",
        "        remaining_text = remaining_text[len(chunk):]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def summarize_text(text):\n",
        "    \n",
        "    prompt = prompt_part_summary.format(text)\n",
        "    response = ChatGPT_conversation([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return response\n",
        "\n",
        "def gpt(text):\n",
        "  prompt = prompt_final_summary.format(text)\n",
        "  response=ChatGPT_conversation([{\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "  return  response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlhKj8tDXHIN"
      },
      "outputs": [],
      "source": [
        "###Audio Creation #####\n",
        "def audio_generation(data):\n",
        "    ###Currently harcoded as we have decided on 4parts.\n",
        "    text = [data[\"Part1\"], data[\"Part2\"], data[\"Part3\"], data[\"Part4\"]] \n",
        "    language = 'en'\n",
        "    audio_list = []\n",
        "\n",
        "    for i, part_text in enumerate(text):\n",
        "        # Passing the text before the pause and language to the engine\n",
        "        tts_before_pause = gTTS(text=part_text, lang=language, slow=False,tld=\"co.in\")\n",
        "\n",
        "        # Save the gTTS object to a temporary file\n",
        "        tts_before_pause.save(f\"temp_audio_{i}.mp3\")\n",
        "\n",
        "        # Convert the gTTS object to an AudioSegment object\n",
        "        audio_before_pause = AudioSegment.from_file(f\"temp_audio_{i}.mp3\")\n",
        "\n",
        "        # Create silent audio segment\n",
        "        silent_duration = 2000\n",
        "        silent_audio = AudioSegment.silent(duration=silent_duration)\n",
        "\n",
        "        # Concatenate silent audio and the generated audio\n",
        "        audio = silent_audio + audio_before_pause\n",
        "\n",
        "        # audio speedup\n",
        "        speedup_factor = 1.25 \n",
        "        audio = audio.speedup(playback_speed=speedup_factor)\n",
        "\n",
        "        # Save the combined and sped-up audio to a file\n",
        "        audio.export(f\"audio_{i}.mp3\", format=\"mp3\")\n",
        "\n",
        "        \n",
        "def generate_frames(text, background_image, frame_duration=5):\n",
        "    frames = []\n",
        "    current_text = \"\"\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        for i in range(0, len(words), words_per_line):\n",
        "            current_line = \" \".join(words[i:i + words_per_line])\n",
        "            current_text += f\"{current_line}\\n\"\n",
        "\n",
        "            image = Image.new('RGB', (width, height), color=(255, 255, 255))\n",
        "            draw = ImageDraw.Draw(image)\n",
        "\n",
        "            font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "            text_width, text_height = draw.textsize(current_text, font=font)\n",
        "            x = width // 2 - text_width // 2\n",
        "            y = height // 2 - text_height // 2\n",
        "\n",
        "            background = Image.open(background_image).convert(\"RGBA\")\n",
        "            image.paste(background, (0, 0), background)\n",
        "\n",
        "            draw.multiline_text((x, y), current_text, font=font, fill=font_color)\n",
        "\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "            image_path = os.path.join(output_folder, f'frame_{len(frames) + 1}.png')\n",
        "            image.save(image_path)\n",
        "            frames.append(image_path)\n",
        "\n",
        "            time.sleep(frame_duration)\n",
        "            current_text += '\\n'\n",
        "\n",
        "    return frames\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9GgzxHGcdPq"
      },
      "outputs": [],
      "source": [
        "# Split audio into chunks\n",
        "audio_chunks = split_audio_into_chunks(media_file_path, chunk_size_seconds)\n",
        "transcription_result = \"\"\n",
        "i=0\n",
        "for chunk in audio_chunks:\n",
        "    transcription_result += transcribe_audio_chunk(openai.api_key, model_id, chunk)\n",
        "    print(i)\n",
        "    i+=1\n",
        "\n",
        "# Clean up temporary directory\n",
        "for chunk in audio_chunks:\n",
        "    os.remove(chunk)\n",
        "os.rmdir('temp_audio_chunks')\n",
        "\n",
        "\n",
        "\n",
        "input_text = transcription_result\n",
        "\n",
        "# Split text into chunks\n",
        "text_chunks = split_text(input_text, input_prompt)\n",
        "\n",
        "# Summarize each chunk\n",
        "summaries = [summarize_text(chunk) for chunk in text_chunks]\n",
        "\n",
        "# Final summary\n",
        "final_summary = '\\n\\n'.join(summaries)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xSiWlcuAI2R"
      },
      "outputs": [],
      "source": [
        "## Summarise Individual summary\n",
        "output = gpt(final_summary)\n",
        "data = json.loads(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH_WpI75bH-W"
      },
      "outputs": [],
      "source": [
        "###Generate Audio for Summary\n",
        "audio_generation(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzV1l6hpMJu8"
      },
      "outputs": [],
      "source": [
        "background_images = \"BG.png\"\n",
        "### Currently hardcoded as we decided on 4 parts.\n",
        "audio_paths = ['audio_0.mp3', 'audio_1.mp3', 'audio_2.mp3', 'audio_3.mp3']\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "### Currently hardcoded as we decided on 4 parts.\n",
        "slides = [\n",
        "    data[\"Part1\"], data[\"Part2\"], data[\"Part3\"], data[\"Part4\"]\n",
        "]\n",
        "\n",
        "final_video_paths = []\n",
        "\n",
        "# Create four videos\n",
        "for i in range(4):\n",
        "    video_path = f'output_video_{i + 1}.mp4'\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video = cv2.VideoWriter(video_path, fourcc, 5, (width, height))\n",
        "\n",
        "    slide_text = slides[i]\n",
        "    frames = generate_frames(slide_text, background_images)\n",
        "\n",
        "    for frame in frames:\n",
        "        img = cv2.imread(frame)\n",
        "        video.write(img)\n",
        "\n",
        "    video.release()\n",
        "\n",
        "    print(f\"Video {i + 1} created at: {video_path}\")\n",
        "\n",
        "    # Add audio to the video\n",
        "    video_clip = VideoFileClip(video_path)\n",
        "    # Select the audio file from the list based on the current index\n",
        "    audio_path = audio_paths[i % len(audio_paths)]\n",
        "    audio_clip = AudioFileClip(audio_path)\n",
        "    video_duration = video_clip.duration\n",
        "    audio_duration = audio_clip.duration\n",
        "\n",
        "    if audio_duration > video_duration:\n",
        "        # Trim video if it's shorter than the audio\n",
        "        video_clip = video_clip.set_duration(audio_duration)\n",
        "    else:\n",
        "        # Trim audio if it's shorter than the video\n",
        "        audio_clip = audio_clip.set_duration(video_duration)\n",
        "\n",
        "    video_clip = video_clip.set_audio(audio_clip)\n",
        "\n",
        "    final_video_path = f'final_output_video_{i + 1}.mp4'\n",
        "    video_clip.write_videofile(final_video_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "    print(f\"Final video {i + 1} created at: {final_video_path}\")\n",
        "\n",
        "    # Remove temporary video file\n",
        "    os.remove(video_path)\n",
        "\n",
        "    # Append the final video path to the list\n",
        "    final_video_paths.append(final_video_path)\n",
        "\n",
        "# Concatenate the final videos into one\n",
        "final_clip = concatenate_videoclips([VideoFileClip(path) for path in final_video_paths], method=\"compose\")\n",
        "final_merge_path = './Lecture_Summary/video1_theory_summary.mp4'\n",
        "final_clip.write_videofile(final_merge_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "print(f\"Final merged video created at: {final_merge_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "command = \"aws s3 cp ./Lecture_Summary/video1_theory_summary.mp4 s3://pw-hackathon-byob/backlog_lectures/theory_summary/video1_theory_summary.mp4\"\n",
        "\n",
        "subprocess.run(command, shell=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Video Processing\n",
        "Creating shorter video based on time frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "from moviepy.editor import concatenate_videoclips\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def convert_to_seconds(time_str):\n",
        "    time_obj = datetime.strptime(time_str, \"%H:%M:%S\")\n",
        "    return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second\n",
        "\n",
        "def cut_and_combine(input_file, output_file, segments):\n",
        "    # List to store video clips\n",
        "    video_clips = []\n",
        "\n",
        "    for start, end in segments:\n",
        "        # Cut the original video into segments\n",
        "        ffmpeg_extract_subclip(input_file, start, end, targetname=f\"temp_{start}_{end}.mp4\")\n",
        "\n",
        "        # Load the cut segment\n",
        "        clip = VideoFileClip(f\"temp_{start}_{end}.mp4\")\n",
        "        video_clips.append(clip)\n",
        "\n",
        "    # Concatenate the video clips\n",
        "    final_clip = concatenate_videoclips(video_clips, method=\"compose\")\n",
        "\n",
        "    # Write the final video to a file\n",
        "    final_clip.write_videofile(output_file, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "    # Clean up temporary files\n",
        "    for start, end in segments:\n",
        "        temp_file = f\"temp_{start}_{end}.mp4\"\n",
        "        os.remove(temp_file)\n",
        "\n",
        "# Example usage with time segments specified as \"hour:minute:second\"\n",
        "# input_video = \"Video1.mp4\"\n",
        "# output_video = \"./Theory_Summary/video1_theory_summary.mp4\"\n",
        "# #output_video = \"./Numerical/video1_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"0:02:06\"), convert_to_seconds(\"0:26:05\")),\n",
        "#                  (convert_to_seconds(\"0:55:06\"), convert_to_seconds(\"1:31:05\"))\n",
        "#                  ]\n",
        "\n",
        "# input_video = \"Video1.mp4\"\n",
        "# #output_video = \"./Theory_Summary/video9_theory_summary.mp4\"\n",
        "# output_video = \"./Numerical/video1_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"0:26:06\"), convert_to_seconds(\"0:55:05\")),\n",
        "#                   (convert_to_seconds(\"1:31:06\"), convert_to_seconds(\"1:37:06\"))\n",
        "                 \n",
        "#                  ]\n",
        "# input_video = \"Video2.mp4\"\n",
        "# #output_video = \"./Theory_Summary/video5_theory_summary.mp4\"\n",
        "# output_video = \"./Numerical/video2_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"0:57:40\"), convert_to_seconds(\"1:02:39\")),\n",
        "#                   (convert_to_seconds(\"1:05:40\"), convert_to_seconds(\"1:11:39\"))\n",
        "#                  ]\n",
        "\n",
        "input_video = \"Video3.mp4\"\n",
        "#output_video = \"./Theory_Summary/video6_theory_summary.mp4\"\n",
        "output_video = \"./Numerical/video3_numericals.mp4\"\n",
        "time_segments = [(convert_to_seconds(\"1:02:33\"), convert_to_seconds(\"1:12:32\")),\n",
        "                  (convert_to_seconds(\"1:33:33\"), convert_to_seconds(\"1:35:32\"))\n",
        "                 \n",
        "                 ]\n",
        "\n",
        "# input_video = \"Video4.mp4\"\n",
        "# output_video = \"./Theory_Summary/video8_theory_summary.mp4\"\n",
        "# #output_video = \"./Numerical/video4_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"0:28:42\"), convert_to_seconds(\"0:30:41\"))\n",
        "                 \n",
        "#                  ]\n",
        "\n",
        "# input_video = \"Video5.mp4\"\n",
        "# output_video = \"./Theory_Summary/video9_theory_summary.mp4\"\n",
        "# #output_video = \"./Numerical/video5_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"1:07:45\"), convert_to_seconds(\"1:11:44\"))\n",
        "                 \n",
        "#                  ]\n",
        "\n",
        "# input_video = \"Video9.mp4\"\n",
        "# output_video = \"./Theory_Summary/video9_theory_summary.mp4\"\n",
        "# #output_video = \"./Numerical/video9_numericals.mp4\"\n",
        "# time_segments = [(convert_to_seconds(\"0:36:40\"), convert_to_seconds(\"0:56:54\"))\n",
        "                 \n",
        "#                  ]\n",
        "\n",
        "\n",
        "# input_video = \"Video10.mp4\"\n",
        "# output_video = \"./Theory_Summary/video10_theory_summary.mp4\"\n",
        "# #output_video = \"./Numerical/video1_numericals.mp4\"\n",
        "# # time_segments = [(convert_to_seconds(\"0:02:40\"), convert_to_seconds(\"0:57:39\")),\n",
        "# #                  (convert_to_seconds(\"1:02:40\"), convert_to_seconds(\"1:05:39\"))\n",
        "# #                  ]\n",
        "# time_segments = [(convert_to_seconds(\"1:12:23\"), convert_to_seconds(\"1:24:31\"))\n",
        "#                 \n",
        "#                  ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cut_and_combine(input_video, output_video, time_segments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
